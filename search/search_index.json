{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+","tags":true},"docs":[{"location":"","text":"<p>Welcome to The Internals of Spark Standalone online book! \ud83e\udd19</p> <p>I'm Jacek Laskowski, an IT freelancer specializing in Apache Spark, Delta Lake and Apache Kafka (with brief forays into a wider data engineering space, e.g. Trino and ksqlDB, mostly during Warsaw Data Engineering meetups).</p> <p>I'm very excited to have you here and hope you will enjoy exploring the internals of Spark Standalone as much as I have.</p>  <p>Flannery O'Connor</p> <p>I write to discover what I know.</p>  \"The Internals Of\" series<p>I'm also writing other online books in the \"The Internals Of\" series. Please visit \"The Internals Of\" Online Books home page.</p>  <p>Expect text and code snippets from a variety of public sources. Attribution follows.</p> <p>Now, let's take a deep dive into Spark Standalone \ud83d\udd25</p>  <p>Last update: 2021-11-01</p>","title":"The Internals of Spark Standalone (Apache Spark 3.2.0)"},{"location":"ClientApp/","text":"<p><code>ClientApp</code> is...FIXME</p>","title":"ClientApp"},{"location":"ClientEndpoint/","text":"<p><code>ClientEndpoint</code> is a <code>ThreadSafeRpcEndpoint</code>.</p>","title":"ClientEndpoint"},{"location":"ClientEndpoint/#creating-instance","text":"<p><code>ClientEndpoint</code> takes the following to be created:</p> <ul> <li> <code>RpcEnv</code>  <p><code>ClientEndpoint</code> is created\u00a0when:</p> <ul> <li><code>StandaloneAppClient</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"CreateSubmissionRequest/","text":"<p><code>CreateSubmissionRequest</code> is a <code>SubmitRestProtocolRequest</code>.</p>","title":"CreateSubmissionRequest"},{"location":"CreateSubmissionRequest/#demo","text":"<p>Start <code>spark-shell</code> and use <code>:paste -raw</code> to paste the following snippet (in raw mode that allows bypassing <code>private[rest]</code> in the classes of our interest).</p> <pre><code>package org.apache.spark.deploy.rest\n\nobject demo {\n  def createCreateSubmissionRequest(): CreateSubmissionRequest = {\n    val submitRequest = new CreateSubmissionRequest\n    submitRequest.clientSparkVersion=\"3.1.2\"\n    submitRequest.sparkProperties=Map(\"spark.app.name\" -&gt; \"DemoApp\")\n    submitRequest.appResource=\"hdfs://localhost:9000/demo-app-1.0.0-jar-with-dependencies.jar\"\n    submitRequest.appArgs=\"this is a demo\".split(\"\\\\s+\")\n    submitRequest.environmentVariables=Map.empty\n    submitRequest.validate\n    submitRequest\n  }\n\n  def client(master: String = \"spark://localhost:6066\"): RestSubmissionClient = { // (1)\n    new RestSubmissionClient(master)\n  }\n}\n</code></pre> <ol> <li>Uses the default <code>6066</code> REST port (not <code>7077</code>)</li> </ol> <pre><code>import org.apache.spark.deploy.rest.demo\nval submitRequest = demo.createCreateSubmissionRequest\nval client = demo.client()\n</code></pre> <pre><code>client.createSubmission(submitRequest)\n</code></pre> <pre><code>scala&gt; println(submitRequest.toJson)\n{\n  \"action\" : \"CreateSubmissionRequest\",\n  \"appArgs\" : [ \"this\", \"is\", \"a\", \"demo\" ],\n  \"appResource\" : \"demo.jar\",\n  \"clientSparkVersion\" : \"3.1.2\",\n  \"environmentVariables\" : { },\n  \"sparkProperties\" : {\n    \"spark.app.name\" : \"DemoApp\"\n  }\n}\n</code></pre>","title":"Demo"},{"location":"ExecutorRunner/","text":"","title":"ExecutorRunner"},{"location":"ExecutorRunner/#creating-instance","text":"<p><code>ExecutorRunner</code> takes the following to be created:</p> <ul> <li> Application ID <li> Executor ID <li> <code>ApplicationDescription</code> <li> Number of CPU cores <li> Amount of Memory <li> <code>RpcEndpointRef</code> <li> Worker ID <li> web UI's scheme <li> Host <li> web UI's port <li> Public Address <li> Spark's Home Directory <li> Executor's Directory <li> Worker's URL <li> <code>SparkConf</code> <li> Local Directories of the Spark Application <li> Executor State <li> <code>Map[String, ResourceInformation]</code> (default: empty)  <p><code>ExecutorRunner</code> is created\u00a0when:</p> <ul> <li><code>Worker</code> is requested to handle a LaunchExecutor message</li> </ul>","title":"Creating Instance"},{"location":"LocalSparkCluster/","text":"<p><code>LocalSparkCluster</code> is a single-JVM Spark Standalone cluster that is available as <code>local-cluster</code> master URL.</p> <p>NOTE: <code>local-cluster</code> master URL matches <code>local-cluster[numWorkers,coresPerWorker,memoryPerWorker]</code> pattern where &lt;&gt;, &lt;&gt; and &lt;&gt; are all numbers separated by the comma. <p><code>LocalSparkCluster</code> can be particularly useful to test distributed operation and fault recovery without spinning up a lot of processes.</p> <p><code>LocalSparkCluster</code> is &lt;&gt; when <code>SparkContext</code> is created for local-cluster master URL (and so requested to xref:ROOT:SparkContext.md#createTaskScheduler[create the SchedulerBackend and the TaskScheduler]). <p>[[logging]] [TIP] ==== Enable <code>INFO</code> logging level for <code>org.apache.spark.deploy.LocalSparkCluster</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.LocalSparkCluster=INFO\n</code></pre>","title":"LocalSparkCluster"},{"location":"LocalSparkCluster/#refer-to-logging","text":"<p>=== [[creating-instance]] Creating LocalSparkCluster Instance</p> <p><code>LocalSparkCluster</code> takes the following when created:</p> <ul> <li>[[numWorkers]] Number of workers</li> <li>[[coresPerWorker]] CPU cores per worker</li> <li>[[memoryPerWorker]] Memory per worker</li> <li>[[conf]] <code>SparkConf</code></li> </ul> <p><code>LocalSparkCluster</code> initializes the &lt;&gt;.","title":"Refer to Logging."},{"location":"LocalSparkCluster/#starting","text":"","title":"Starting <pre><code>start(): Array[String]\n</code></pre> <p><code>start</code>...FIXME</p> <p><code>start</code> is used when...FIXME</p> <p>=== [[stop]] Stopping LocalSparkCluster</p>"},{"location":"LocalSparkCluster/#source-scala","text":"","title":"[source, scala]"},{"location":"LocalSparkCluster/#stop-unit","text":"","title":"stop(): Unit <p><code>stop</code>...FIXME</p> <p>NOTE: <code>stop</code> is used when...FIXME</p>"},{"location":"Master/","text":"<p><code>Master</code> is the manager of a Spark Standalone cluster.</p> <p><code>Master</code> can be launched from command line.</p>","title":"Master"},{"location":"Master/#standalonerestserver","text":"","title":"StandaloneRestServer <p><code>Master</code> can start StandaloneRestServer when enabled using spark.master.rest.enabled configuration property.</p> <p><code>StandaloneRestServer</code> is requested to start in onStart and stop in onStop</p>"},{"location":"Master/#master-rpc-endpoint","text":"","title":"Master RPC Endpoint <p><code>Master</code> is a <code>ThreadSafeRpcEndpoint</code> and is registered under Master name (when launched as a command-line application and requested to start up an RPC environment).</p>"},{"location":"Master/#launching-standalone-master","text":"","title":"Launching Standalone Master <p><code>Master</code> can be launched as a standalone application using <code>spark-class</code>.</p> <pre><code>./bin/spark-class org.apache.spark.deploy.master.Master\n</code></pre>"},{"location":"Master/#main-entry-point","text":"","title":"main Entry Point <pre><code>main(\n  argStrings: Array[String]): Unit\n</code></pre> <p><code>main</code> is the entry point of the <code>Master</code> standalone application.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Started daemon with process name: [processName]\n</code></pre> <p><code>main</code> registers signal handlers for <code>TERM</code>, <code>HUP</code>, <code>INT</code> signals.</p> <p><code>main</code> parses command-line options (using <code>MasterArguments</code>) and initializes an RpcEnv.</p> <p>In the end, <code>main</code> requests the <code>RpcEnv</code> to be notified when <code>terminated</code>.</p>"},{"location":"Master/#command-line-options","text":"","title":"Command-Line Options <p><code>Master</code> supports command-line options.</p> <pre><code>Usage: Master [options]\n\nOptions:\n  -i HOST, --ip HOST     Hostname to listen on (deprecated, please use --host or -h)\n  -h HOST, --host HOST   Hostname to listen on\n  -p PORT, --port PORT   Port to listen on (default: 7077)\n  --webui-port PORT      Port for web UI (default: 8080)\n  --properties-file FILE Path to a custom Spark properties file.\n                         Default is conf/spark-defaults.conf.\n</code></pre>"},{"location":"Master/#host","text":"","title":"host"},{"location":"Master/#ip","text":"","title":"ip"},{"location":"Master/#port","text":"","title":"port"},{"location":"Master/#properties-file","text":"","title":"properties-file"},{"location":"Master/#webui-port","text":"","title":"webui-port"},{"location":"Master/#creating-instance","text":"","title":"Creating Instance <p><code>Master</code> takes the following to be created:</p> <ul> <li> <code>RpcEnv</code> <li> <code>RpcAddress</code> <li> web UI's Port <li> <code>SecurityManager</code> <li> <code>SparkConf</code>  <p><code>Master</code> is created\u00a0when:</p> <ul> <li><code>Master</code> utility is requested to start up RPC environment</li> </ul>"},{"location":"Master/#starting-up-rpc-environment","text":"","title":"Starting Up RPC Environment <pre><code>startRpcEnvAndEndpoint(\n  host: String,\n  port: Int,\n  webUiPort: Int,\n  conf: SparkConf): (RpcEnv, Int, Option[Int])\n</code></pre> <p><code>startRpcEnvAndEndpoint</code> creates a <code>RpcEnv</code> with sparkMaster name (and the input arguments) and registers Master endpoint with Master name.</p> <p>In the end, <code>startRpcEnvAndEndpoint</code> sends <code>BoundPortsResponse</code> message (synchronously) to the Master endpoint and returns the <code>RpcEnv</code> with the ports of the web UI and the REST Server.</p> <p></p> <p><code>startRpcEnvAndEndpoint</code>\u00a0is used when:</p> <ul> <li><code>LocalSparkCluster</code> is requested to start</li> <li><code>Master</code> standalone application is launched</li> </ul>"},{"location":"Master/#spreadoutapps","text":"","title":"spreadOutApps <p><code>Master</code> uses spark.deploy.spreadOut configuration property when requested to startExecutorsOnWorkers.</p>"},{"location":"Master/#scheduling-resources-among-waiting-applications","text":"","title":"Scheduling Resources Among Waiting Applications <pre><code>schedule(): Unit\n</code></pre> <p><code>schedule</code>...FIXME</p> <p><code>schedule</code> is used when:</p> <ul> <li><code>Master</code> is requested to schedule resources among waiting applications</li> </ul>"},{"location":"Master/#startexecutorsonworkers","text":"","title":"startExecutorsOnWorkers <pre><code>startExecutorsOnWorkers(): Unit\n</code></pre> <p><code>startExecutorsOnWorkers</code>...FIXME</p>"},{"location":"Master/#webui","text":"","title":"WebUI <p><code>MasterWebUI</code> is the Web UI server for the standalone master. Master starts Web UI to listen to <code>http://[master's hostname]:webUIPort</code> (e.g. <code>http://localhost:8080</code>).</p> <pre><code>Successfully started service 'MasterUI' on port 8080.\nStarted MasterWebUI at http://192.168.1.4:8080\n</code></pre>"},{"location":"Master/#states","text":"","title":"States <p>Master can be in the following states:</p> <ul> <li><code>STANDBY</code> - the initial state while <code>Master</code> is initializing</li> <li><code>ALIVE</code> - start scheduling resources among applications</li> <li><code>RECOVERING</code></li> <li><code>COMPLETING_RECOVERY</code></li> </ul>"},{"location":"Master/#leaderelectable","text":"","title":"LeaderElectable <p><code>Master</code> is <code>LeaderElectable</code>.</p>"},{"location":"Master/#to-be-reviewed","text":"","title":"To be Reviewed <p>Application ids follows the pattern <code>app-yyyyMMddHHmmss</code>.</p> <p><code>Master</code> can be &lt;&gt; and stopped using link:spark-standalone-master-scripts.md[custom management scripts for standalone Master]."},{"location":"Master/#rest-server","text":"","title":"REST Server <p>The standalone Master starts the REST Server service for alternative application submission that is supposed to work across Spark versions. It is enabled by default (see &lt;&gt;) and used by link:spark-submit.md[spark-submit] for the link:spark-standalone.md#deployment-modes[standalone cluster mode], i.e. <code>--deploy-mode</code> is <code>cluster</code>. <p><code>RestSubmissionClient</code> is the client.</p> <p>The server includes a JSON representation of <code>SubmitRestProtocolResponse</code> in the HTTP body.</p> <p>The following INFOs show up when the Master Endpoint starts up (<code>Master#onStart</code> is called) with REST Server enabled:</p> <pre><code>INFO Utils: Successfully started service on port 6066.\nINFO StandaloneRestServer: Started REST server for submitting applications on port 6066\n</code></pre>"},{"location":"Master/#recovery-mode","text":"","title":"Recovery Mode <p>A standalone Master can run with recovery mode enabled and be able to recover state among the available swarm of masters. By default, there is no recovery, i.e. no persistence and no election.</p> <p>NOTE: Only a master can schedule tasks so having one always on is important for cases where you want to launch new tasks. Running tasks are unaffected by the state of the master.</p> <p>Master uses <code>spark.deploy.recoveryMode</code> to set up the recovery mode (see &lt;&gt;). <p>The Recovery Mode enables &lt;&gt; among the masters. <p>TIP: Check out the exercise link:exercises/spark-exercise-standalone-master-ha.md[Spark Standalone - Using ZooKeeper for High-Availability of Master].</p>"},{"location":"Master/#rpc-messages","text":"","title":"RPC Messages <p>Master communicates with drivers, executors and configures itself using RPC messages.</p> <p>The following message types are accepted by master (see <code>Master#receive</code> or <code>Master#receiveAndReply</code> methods):</p> <ul> <li><code>ElectedLeader</code> for &lt;&gt; <li><code>CompleteRecovery</code></li> <li><code>RevokedLeadership</code></li> <li>&lt;&gt; <li><code>ExecutorStateChanged</code></li> <li><code>DriverStateChanged</code></li> <li><code>Heartbeat</code></li> <li><code>MasterChangeAcknowledged</code></li> <li><code>WorkerSchedulerStateResponse</code></li> <li><code>UnregisterApplication</code></li> <li><code>CheckForWorkerTimeOut</code></li> <li><code>RegisterWorker</code></li> <li><code>RequestSubmitDriver</code></li> <li><code>RequestKillDriver</code></li> <li><code>RequestDriverStatus</code></li> <li><code>RequestMasterState</code></li> <li><code>BoundPortsRequest</code></li> <li><code>RequestExecutors</code></li> <li><code>KillExecutors</code></li>"},{"location":"Master/#registerapplication-event","text":"<p>A RegisterApplication event is sent by link:spark-standalone.md#AppClient[AppClient] to the standalone Master. The event holds information about the application being deployed (<code>ApplicationDescription</code>) and the driver's endpoint reference.</p> <p><code>ApplicationDescription</code> describes an application by its name, maximum number of cores, executor's memory, command, appUiUrl, and user with optional eventLogDir and eventLogCodec for Event Logs, and the number of cores per executor.</p> <p>CAUTION: FIXME Finish</p> <p>A standalone Master receives <code>RegisterApplication</code> with a <code>ApplicationDescription</code> and the driver's xref:rpc:RpcEndpointRef.md[RpcEndpointRef].</p> <pre><code>INFO Registering app \" + description.name\n</code></pre> <p>Application ids in Spark Standalone are in the format of <code>app-[yyyyMMddHHmmss]-[4-digit nextAppNumber]</code>.</p> <p>Master keeps track of the number of already-scheduled applications (<code>nextAppNumber</code>).</p> <p>ApplicationDescription (AppClient) \u2192 ApplicationInfo (Master) - application structure enrichment</p> <p><code>ApplicationSource</code> metrics + <code>applicationMetricsSystem</code></p> <pre><code>INFO Registered app \" + description.name + \" with ID \" + app.id\n</code></pre> <p>CAUTION: FIXME <code>persistenceEngine.addApplication(app)</code></p> <p><code>schedule()</code> schedules the currently available resources among waiting apps.</p> <p>FIXME When is <code>schedule()</code> method called?</p> <p>It's only executed when the Master is in <code>RecoveryState.ALIVE</code> state.</p> <p>Worker in <code>WorkerState.ALIVE</code> state can accept applications.</p> <p>A driver has a state, i.e. <code>driver.state</code> and when it's in <code>DriverState.RUNNING</code> state the driver has been assigned to a worker for execution.</p>","title":"RegisterApplication event"},{"location":"Master/#launchdriver-rpc-message","text":"<p>WARNING: It seems a dead message. Disregard it for now.</p> <p>A LaunchDriver message is sent by an active standalone Master to a worker to launch a driver.</p> <p>.Master finds a place for a driver (posts LaunchDriver) image::spark-standalone-master-worker-LaunchDriver.png[align=\"center\"]</p> <p>You should see the following INFO in the logs right before the message is sent out to a worker:</p> <pre><code>INFO Launching driver [driver.id] on worker [worker.id]\n</code></pre> <p>The message holds information about the id and name of the driver.</p> <p>A driver can be running on a single worker while a worker can have many drivers running.</p> <p>When a worker receives a <code>LaunchDriver</code> message, it prints out the following INFO:</p> <pre><code>Asked to launch driver [driver.id]\n</code></pre> <p>It then creates a <code>DriverRunner</code> and starts it. It starts a separate JVM process.</p> <p>Workers' free memory and cores are considered when assigning some to waiting drivers (applications).</p> <p>CAUTION: FIXME Go over <code>waitingDrivers</code>...</p>","title":"LaunchDriver RPC message"},{"location":"Master/#internals-of-orgapachesparkdeploymastermaster","text":"","title":"Internals of org.apache.spark.deploy.master.Master <p>When <code>Master</code> starts, it first creates the default <code>SparkConf</code> configuration whose values it then overrides using  &lt;&gt; and &lt;&gt;. <p>A fully-configured master instance requires <code>host</code>, <code>port</code> (default: <code>7077</code>), <code>webUiPort</code> (default: <code>8080</code>) settings defined.</p> <p>TIP: When in troubles, consult link:spark-tips-and-tricks.md[Spark Tips and Tricks] document.</p> <p>It starts &lt;&gt; with necessary endpoints and lives until the RPC environment terminates."},{"location":"Master/#worker-management","text":"","title":"Worker Management <p>Master uses <code>master-forward-message-thread</code> to schedule a thread every <code>spark.worker.timeout</code> to check workers' availability and remove timed-out workers.</p> <p>It is that Master sends <code>CheckForWorkerTimeOut</code> message to itself to trigger verification.</p> <p>When a worker hasn't responded for <code>spark.worker.timeout</code>, it is assumed dead and the following WARN message appears in the logs:</p> <pre><code>WARN Removing [worker.id] because we got no heartbeat in [spark.worker.timeout] seconds\n</code></pre>"},{"location":"Master/#system-environment-variables","text":"","title":"System Environment Variables <p>Master uses the following system environment variables (directly or indirectly):</p> <ul> <li><code>SPARK_LOCAL_HOSTNAME</code> - the custom host name</li> <li><code>SPARK_LOCAL_IP</code> - the custom IP to use when <code>SPARK_LOCAL_HOSTNAME</code> is not set</li> <li><code>SPARK_MASTER_HOST</code> (not <code>SPARK_MASTER_IP</code> as used in <code>start-master.sh</code> script above!) - the master custom host</li> <li><code>SPARK_MASTER_PORT</code> (default: <code>7077</code>) - the master custom port</li> <li><code>SPARK_MASTER_IP</code> (default: <code>hostname</code> command's output)</li> <li><code>SPARK_MASTER_WEBUI_PORT</code> (default: <code>8080</code>) - the port of the master's WebUI. Overriden by <code>spark.master.ui.port</code> if set in the properties file.</li> <li><code>SPARK_PUBLIC_DNS</code> (default: hostname) - the custom master hostname for WebUI's http URL and master's address.</li> <li><code>SPARK_CONF_DIR</code> (default: <code>$SPARK_HOME/conf</code>) - the directory of the default properties file link:spark-properties.md#spark-defaults-conf[spark-defaults.conf] from which all properties that start with <code>spark.</code> prefix are loaded.</li> </ul>"},{"location":"Master/#settings","text":"","title":"Settings <p>Master uses the following properties:</p> <ul> <li><code>spark.cores.max</code> (default: <code>0</code>) - total expected number of cores. When set, an application could get executors of different sizes (in terms of cores).</li> <li><code>spark.dead.worker.persistence</code> (default: <code>15</code>)</li> <li><code>spark.deploy.retainedApplications</code> (default: <code>200</code>)</li> <li><code>spark.deploy.retainedDrivers</code> (default: <code>200</code>)</li> <li><code>spark.deploy.recoveryMode</code> (default: <code>NONE</code>) - possible modes: <code>ZOOKEEPER</code>, <code>FILESYSTEM</code>, or <code>CUSTOM</code>. Refer to &lt;&gt;. <li><code>spark.deploy.recoveryMode.factory</code> - the class name of the custom <code>StandaloneRecoveryModeFactory</code>.</li> <li><code>spark.deploy.recoveryDirectory</code> (default: empty) - the directory to persist recovery state</li> <li>link:spark-standalone.md#spark.deploy.spreadOut[spark.deploy.spreadOut] to perform link:spark-standalone.md#round-robin-scheduling[round-robin scheduling across the nodes].</li> <li><code>spark.deploy.defaultCores</code> (default: <code>Int.MaxValue</code>, i.e. unbounded) - the number of maxCores for applications that don't specify it.</li> <li><code>spark.worker.timeout</code> (default: <code>60</code>) - time (in seconds) when no heartbeat from a worker means it is lost. See &lt;&gt;."},{"location":"RestSubmissionClient/","text":"","title":"RestSubmissionClient"},{"location":"RestSubmissionClient/#creating-instance","text":"<p><code>RestSubmissionClient</code> takes the following to be created:</p> <ul> <li> Master URL  <p><code>RestSubmissionClient</code> is created\u00a0when:</p> <ul> <li><code>SparkSubmit</code> is requested to <code>kill</code> and <code>requestStatus</code></li> <li><code>RestSubmissionClientApp</code> is requested to <code>run</code></li> </ul>","title":"Creating Instance"},{"location":"RestSubmissionClient/#createsubmission","text":"","title":"createSubmission <pre><code>createSubmission(\n  request: CreateSubmissionRequest): SubmitRestProtocolResponse\n</code></pre> <p><code>createSubmission</code> prints out the following INFO message to the logs (with the master URL):</p> <pre><code>Submitting a request to launch an application in [master].\n</code></pre> <p><code>createSubmission</code>...FIXME</p> <p><code>createSubmission</code>\u00a0is used when:</p> <ul> <li>FIXME</li> </ul>"},{"location":"RestSubmissionClient/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.rest.RestSubmissionClient</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.rest.RestSubmissionClient=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"RestSubmissionServer/","text":"<p><code>RestSubmissionServer</code> is an abstraction of Application Submission Gateways that can handle submit, kill and status requests using REST API (JSON over HTTP).</p>","title":"RestSubmissionServer"},{"location":"RestSubmissionServer/#urls-and-restservlets","text":"","title":"URLs and RestServlets    URL Method RestServlet     /v1/submissions/create/* POST SubmitRequestServlet   /v1/submissions/kill/* POST KillRequestServlet   /v1/submissions/status/* GET StatusRequestServlet   /* (all) ErrorServlet    <p>The above URLs and <code>RestServlet</code>s are registered when <code>RestSubmissionServer</code> is requested to start.</p>"},{"location":"RestSubmissionServer/#contract","text":"","title":"Contract"},{"location":"RestSubmissionServer/#killrequestservlet","text":"","title":"killRequestServlet <pre><code>killRequestServlet: KillRequestServlet\n</code></pre> <p>Used when:</p> <ul> <li><code>RestSubmissionServer</code> is requested for the contextToServlet</li> </ul>"},{"location":"RestSubmissionServer/#statusrequestservlet","text":"","title":"statusRequestServlet <pre><code>statusRequestServlet: StatusRequestServlet\n</code></pre> <p>Used when:</p> <ul> <li><code>RestSubmissionServer</code> is requested for the contextToServlet</li> </ul>"},{"location":"RestSubmissionServer/#submitrequestservlet","text":"","title":"submitRequestServlet <pre><code>submitRequestServlet: SubmitRequestServlet\n</code></pre> <p>Used when:</p> <ul> <li><code>RestSubmissionServer</code> is requested for the contextToServlet</li> </ul>"},{"location":"RestSubmissionServer/#implementations","text":"","title":"Implementations <ul> <li>MesosRestServer</li> <li>StandaloneRestServer</li> </ul>"},{"location":"RestSubmissionServer/#creating-instance","text":"","title":"Creating Instance <p><code>RestSubmissionServer</code> takes the following to be created:</p> <ul> <li> Host name <li> Requested Port <li> <code>SparkConf</code>  Abstract Class<p><code>RestSubmissionServer</code>\u00a0is an abstract class and cannot be created directly. It is created indirectly for the concrete RestSubmissionServers.</p>"},{"location":"RestSubmissionServer/#starting","text":"","title":"Starting <pre><code>start(): Int\n</code></pre> <p><code>start</code> starts a REST service on the requested port (or any free higher).</p> <p><code>start</code> prints out the following INFO to the logs:</p> <pre><code>Started REST server for submitting applications on port [port]\n</code></pre> <p>In the end, <code>start</code> returns the port of the server.</p> <p><code>start</code>\u00a0is used when:</p> <ul> <li><code>Master</code> (Spark Standalone) is requested to onStart</li> </ul>"},{"location":"RestSubmissionServer/#dostart","text":"","title":"doStart <pre><code>doStart(\n  startPort: Int): (Server, Int)\n</code></pre> <p><code>doStart</code>...FIXME</p>"},{"location":"RestSubmissionServer/#logging","text":"","title":"Logging <p><code>RestSubmissionServer</code> is an abstract class and logging is configured using the logger of the implementations.</p>"},{"location":"StandaloneAppClient/","text":"","title":"StandaloneAppClient"},{"location":"StandaloneAppClient/#creating-instance","text":"<p><code>StandaloneAppClient</code> takes the following to be created:</p> <ul> <li> <code>RpcEnv</code> <li> Master URLs <li> <code>ApplicationDescription</code> <li> StandaloneAppClientListener <li> <code>SparkConf</code>  <p><code>StandaloneAppClient</code> is created\u00a0when:</p> <ul> <li><code>StandaloneSchedulerBackend</code> is requested to start</li> </ul>","title":"Creating Instance"},{"location":"StandaloneAppClient/#starting","text":"","title":"Starting <pre><code>start(): Unit\n</code></pre> <p><code>start</code> registers a ClientEndpoint under the name AppClient (and saves it as the RpcEndpointRef).</p> <p><code>start</code>\u00a0is used when:</p> <ul> <li><code>StandaloneSchedulerBackend</code> is requested to start</li> </ul>"},{"location":"StandaloneAppClient/#stopping","text":"","title":"Stopping <pre><code>stop(): Unit\n</code></pre> <p><code>stop</code>...FIXME</p> <p><code>stop</code>\u00a0is used when:</p> <ul> <li><code>StandaloneSchedulerBackend</code> is requested to stop</li> </ul>"},{"location":"StandaloneAppClientListener/","text":"<p><code>StandaloneAppClientListener</code> is an abstraction of listeners.</p>","title":"StandaloneAppClientListener"},{"location":"StandaloneAppClientListener/#contract","text":"","title":"Contract"},{"location":"StandaloneAppClientListener/#connected","text":"","title":"connected <pre><code>connected(\n  appId: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to handle a RegisteredApplication message</li> </ul>"},{"location":"StandaloneAppClientListener/#dead","text":"","title":"dead <pre><code>dead(\n  reason: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to markDead</li> </ul>"},{"location":"StandaloneAppClientListener/#disconnected","text":"","title":"disconnected <pre><code>disconnected(): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to markDisconnected</li> </ul>"},{"location":"StandaloneAppClientListener/#executoradded","text":"","title":"executorAdded <pre><code>executorAdded(\n  fullId: String,\n  workerId: String,\n  hostPort: String,\n  cores: Int,\n  memory: Int): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to handle a ExecutorAdded message</li> </ul>"},{"location":"StandaloneAppClientListener/#executordecommissioned","text":"","title":"executorDecommissioned <pre><code>executorDecommissioned(\n  fullId: String,\n  decommissionInfo: ExecutorDecommissionInfo): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to handle a ExecutorUpdated message</li> </ul>"},{"location":"StandaloneAppClientListener/#executorremoved","text":"","title":"executorRemoved <pre><code>executorRemoved(\n  fullId: String,\n  message: String,\n  exitStatus: Option[Int],\n  workerHost: Option[String]): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to handle a ExecutorUpdated message</li> </ul>"},{"location":"StandaloneAppClientListener/#workerremoved","text":"","title":"workerRemoved <pre><code>workerRemoved(\n  workerId: String,\n  host: String,\n  message: String): Unit\n</code></pre> <p>Used when:</p> <ul> <li><code>ClientEndpoint</code> is requested to handle a WorkerRemoved message</li> </ul>"},{"location":"StandaloneAppClientListener/#implementations","text":"<ul> <li>StandaloneSchedulerBackend</li> </ul>","title":"Implementations"},{"location":"StandaloneRestServer/","text":"<p><code>StandaloneRestServer</code> is a RestSubmissionServer.</p>","title":"StandaloneRestServer"},{"location":"StandaloneRestServer/#creating-instance","text":"<p><code>StandaloneRestServer</code> takes the following to be created:</p> <ul> <li> Host Name <li> Requested Port <li> <code>SparkConf</code> <li> <code>RpcEndpointRef</code> of the Master <li> URL of the Master  <p><code>StandaloneRestServer</code> is created\u00a0when:</p> <ul> <li><code>Master</code> is requested to onStart (with spark.master.rest.enabled configuration property enabled)</li> </ul>","title":"Creating Instance"},{"location":"StandaloneRestServer/#standalonesubmitrequestservlet","text":"","title":"StandaloneSubmitRequestServlet <p><code>StandaloneRestServer</code> uses a <code>StandaloneSubmitRequestServlet</code> as the submitRequestServlet.</p> <p><code>StandaloneSubmitRequestServlet</code> requires the following parameters:</p> <ul> <li><code>appResource</code> (the jar of a Spark application)</li> <li><code>mainClass</code></li> </ul>"},{"location":"StandaloneSchedulerBackend/","text":"<p><code>StandaloneSchedulerBackend</code> is a <code>CoarseGrainedSchedulerBackend</code>.</p>","title":"StandaloneSchedulerBackend"},{"location":"StandaloneSchedulerBackend/#creating-instance","text":"<p><code>StandaloneSchedulerBackend</code> takes the following to be created:</p> <ul> <li> <code>TaskSchedulerImpl</code> <li> <code>SparkContext</code> <li> Standalone master URLs  <p><code>StandaloneSchedulerBackend</code> is created\u00a0when:</p> <ul> <li><code>SparkContext</code> is requested for a <code>SchedulerBackend</code> (and <code>TaskScheduler</code>) (for <code>spark://</code> and <code>local-cluster</code> master URLs)</li> </ul>","title":"Creating Instance"},{"location":"StandaloneSchedulerBackend/#standaloneappclientlistener","text":"","title":"StandaloneAppClientListener <p><code>StandaloneSchedulerBackend</code> is a StandaloneAppClientListener.</p>"},{"location":"StandaloneSchedulerBackend/#starting-schedulerbackend","text":"","title":"Starting SchedulerBackend <pre><code>start(): Unit\n</code></pre> <p><code>start</code>...FIXME</p> <p><code>start</code> creates a StandaloneAppClient and requests it to start.</p> <p><code>start</code>...FIXME</p> <p><code>start</code> is part of the <code>SchedulerBackend</code> abstraction.</p>"},{"location":"StandaloneSchedulerBackend/#standaloneappclient","text":"","title":"StandaloneAppClient <p><code>StandaloneSchedulerBackend</code> creates a StandaloneAppClient (with itself as a StandaloneAppClientListener) when requested to start.</p> <p><code>StandaloneAppClient</code> is started with StandaloneSchedulerBackend.</p> <p><code>StandaloneAppClient</code> is stopped with StandaloneSchedulerBackend.</p> <p><code>StandaloneAppClient</code> is used for the following:</p> <ul> <li>doRequestTotalExecutors</li> <li>doKillExecutors</li> </ul>"},{"location":"Worker/","text":"<p><code>Worker</code> is a logical worker node in a Spark Standalone cluster.</p> <p><code>Worker</code> can be launched from command line.</p>","title":"Worker"},{"location":"Worker/#worker-rpc-endpoint","text":"","title":"Worker RPC Endpoint <p><code>Worker</code> is a <code>ThreadSafeRpcEndpoint</code> and is registered under Worker name (when launched as a command-line application and requested to set up an RPC environment).</p>"},{"location":"Worker/#launching-standalone-worker","text":"","title":"Launching Standalone Worker <p><code>Worker</code> can be launched as a standalone application using <code>spark-class</code>.</p> <pre><code>./bin/spark-class org.apache.spark.deploy.worker.Worker\n</code></pre>  <p>Note</p> <p>At least one master URL is required.</p>"},{"location":"Worker/#main-entry-point","text":"","title":"main Entry Point <pre><code>main(\n  args: Array[String]): Unit\n</code></pre> <p><code>main</code> is the entry point of <code>Worker</code> standalone application.</p> <p><code>main</code> prints out the following INFO message to the logs:</p> <pre><code>Started daemon with process name: [processName]\n</code></pre> <p><code>main</code> registers signal handlers for <code>TERM</code>, <code>HUP</code>, <code>INT</code> signals.</p> <p><code>main</code> parses command-line options (using <code>WorkerArguments</code>) and initializes an RpcEnv.</p> <p><code>main</code> asserts that:</p> <ol> <li>External Shuffle Service is not used (based on <code>spark.shuffle.service.enabled</code> configuration property)</li> <li>Number of worker instances is <code>1</code> (based on <code>SPARK_WORKER_INSTANCES</code> environment variable)</li> </ol> <p><code>main</code> throws an <code>IllegalArgumentException</code> when the above does not hold:</p> <pre><code>Starting multiple workers on one host is failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict.\n</code></pre> <p>In the end, <code>main</code> requests the <code>RpcEnv</code> to be notified when <code>terminated</code>.</p>"},{"location":"Worker/#command-line-options","text":"","title":"Command-Line Options <p><code>Worker</code> supports command-line options.</p> <pre><code>Usage: Worker [options] &lt;master&gt;\n\nMaster must be a URL of the form spark://hostname:port\n\nOptions:\n  -c CORES, --cores CORES  Number of cores to use\n  -m MEM, --memory MEM     Amount of memory to use (e.g. 1000M, 2G)\n  -d DIR, --work-dir DIR   Directory to run apps in (default: SPARK_HOME/work)\n  -i HOST, --ip IP         Hostname to listen on (deprecated, please use --host or -h)\n  -h HOST, --host HOST     Hostname to listen on\n  -p PORT, --port PORT     Port to listen on (default: random)\n  --webui-port PORT        Port for web UI (default: 8081)\n  --properties-file FILE   Path to a custom Spark properties file.\n                           Default is conf/spark-defaults.conf.\n</code></pre>"},{"location":"Worker/#cores","text":"","title":"cores"},{"location":"Worker/#host","text":"","title":"host"},{"location":"Worker/#ip","text":"","title":"ip"},{"location":"Worker/#master-urls","text":"","title":"Master URLs <p>(required) Comma-separated standalone Master's URLs in the form:</p> <pre><code>spark://host1:port1,host2:port2,...\n</code></pre>"},{"location":"Worker/#memory","text":"","title":"memory"},{"location":"Worker/#port","text":"","title":"port"},{"location":"Worker/#properties-file","text":"","title":"properties-file"},{"location":"Worker/#webui-port","text":"","title":"webui-port"},{"location":"Worker/#work-dir","text":"","title":"work-dir"},{"location":"Worker/#creating-instance","text":"","title":"Creating Instance <p><code>Worker</code> takes the following to be created:</p> <ul> <li> <code>RpcEnv</code> <li> web UI's Port <li> Number of CPU cores <li> Memory <li> <code>RpcAddress</code>es of the Masters <li> Endpoint Name <li> Work Dir Path (default: <code>null</code>) <li> <code>SparkConf</code> <li> <code>SecurityManager</code> <li> Optional Resource File (default: (undefined)) <li> Supplier of <code>ExternalShuffleService</code> (default: <code>null</code>)  <p><code>Worker</code> is created\u00a0when:</p> <ul> <li><code>Worker</code> utility is requested to startRpcEnvAndEndpoint</li> </ul>"},{"location":"Worker/#externalshuffleservice","text":"","title":"ExternalShuffleService <p><code>Worker</code> initializes an <code>ExternalShuffleService</code> (directly or indirectly using a Supplier if given).</p> <p><code>ExternalShuffleService</code> is <code>started</code> when <code>Worker</code> is requested to startExternalShuffleService.</p> <p><code>ExternalShuffleService</code> is used as follows:</p> <ul> <li> <p>Informed about an application removed when <code>Worker</code> handles a WorkDirCleanup message or maybeCleanupApplication</p> </li> <li> <p>Informed about an executor removed when <code>Worker</code> is requested to handleExecutorStateChanged</p> </li> </ul> <p><code>ExternalShuffleService</code> is stopped when <code>Worker</code> is requested to stop.</p>"},{"location":"Worker/#starting-up-rpc-environment","text":"","title":"Starting Up RPC Environment <pre><code>startRpcEnvAndEndpoint(\n  host: String,\n  port: Int,\n  webUiPort: Int,\n  cores: Int,\n  memory: Int,\n  masterUrls: Array[String],\n  workDir: String,\n  workerNumber: Option[Int] = None,\n  conf: SparkConf = new SparkConf,\n  resourceFileOpt: Option[String] = None): RpcEnv\n</code></pre> <p><code>startRpcEnvAndEndpoint</code> creates an <code>RpcEnv</code> (with the name <code>sparkWorker</code> and the given <code>host</code> and <code>port</code>).</p> <p><code>startRpcEnvAndEndpoint</code> translates the given <code>masterUrls</code> to <code>RpcAddress</code>es.</p> <p><code>startRpcEnvAndEndpoint</code> creates a Worker and requests the <code>RpcEnv</code> to set it up as an RPC endpoint under the Worker name.</p> <p><code>startRpcEnvAndEndpoint</code>\u00a0is used when:</p> <ul> <li><code>LocalSparkCluster</code> is requested to <code>start</code></li> <li><code>Worker</code> standalone application is launched</li> </ul>"},{"location":"Worker/#creating-work-directory","text":"","title":"Creating Work Directory <pre><code>createWorkDir(): Unit\n</code></pre> <p><code>createWorkDir</code> sets &lt;&gt; to be either &lt;&gt; if defined or &lt;&gt; with <code>work</code> subdirectory. <p>In the end, <code>createWorkDir</code> creates &lt;&gt; directory (including any necessary but nonexistent parent directories). <p><code>createWorkDir</code> reports...FIXME</p>"},{"location":"Worker/#messages","text":"","title":"Messages"},{"location":"Worker/#applicationfinished","text":"","title":"ApplicationFinished"},{"location":"Worker/#driverstatechanged","text":"","title":"DriverStateChanged"},{"location":"Worker/#executorstatechanged","text":"","title":"ExecutorStateChanged <pre><code>ExecutorStateChanged(\n  appId: String,\n  execId: Int,\n  state: ExecutorState,\n  message: Option[String],\n  exitStatus: Option[Int])\n</code></pre> <p>Message Handler: handleExecutorStateChanged</p> <p>Posted when:</p> <ul> <li><code>ExecutorRunner</code> is requested to killProcess and fetchAndRunExecutor</li> </ul>"},{"location":"Worker/#killdriver","text":"","title":"KillDriver"},{"location":"Worker/#killexecutor","text":"","title":"KillExecutor"},{"location":"Worker/#launchdriver","text":"","title":"LaunchDriver"},{"location":"Worker/#launchexecutor","text":"","title":"LaunchExecutor"},{"location":"Worker/#masterchanged","text":"","title":"MasterChanged"},{"location":"Worker/#reconnectworker","text":"","title":"ReconnectWorker"},{"location":"Worker/#registerworkerresponse","text":"","title":"RegisterWorkerResponse"},{"location":"Worker/#reregisterwithmaster","text":"","title":"ReregisterWithMaster"},{"location":"Worker/#requestworkerstate","text":"","title":"RequestWorkerState"},{"location":"Worker/#sendheartbeat","text":"","title":"SendHeartbeat"},{"location":"Worker/#workdircleanup","text":"","title":"WorkDirCleanup"},{"location":"Worker/#handleexecutorstatechanged","text":"","title":"handleExecutorStateChanged <pre><code>handleExecutorStateChanged(\n  executorStateChanged: ExecutorStateChanged): Unit\n</code></pre> <p><code>handleExecutorStateChanged</code>...FIXME</p> <p><code>handleExecutorStateChanged</code> is used when:</p> <ul> <li><code>Worker</code> is requested to handle ExecutorStateChanged message</li> </ul>"},{"location":"Worker/#maybecleanupapplication","text":"","title":"maybeCleanupApplication <pre><code>maybeCleanupApplication(\n  id: String): Unit\n</code></pre> <p><code>maybeCleanupApplication</code>...FIXME</p> <p><code>maybeCleanupApplication</code> is used when:</p> <ul> <li><code>Worker</code> is requested to handle a ApplicationFinished message and handleExecutorStateChanged</li> </ul>"},{"location":"Worker/#logging","text":"","title":"Logging <p>Enable <code>ALL</code> logging level for <code>org.apache.spark.deploy.worker.Worker</code> logger to see what happens inside.</p> <p>Add the following line to <code>conf/log4j.properties</code>:</p> <pre><code>log4j.logger.org.apache.spark.deploy.worker.Worker=ALL\n</code></pre> <p>Refer to Logging.</p>"},{"location":"configuration-properties/","text":"","title":"Configuration Properties of Spark Standalone"},{"location":"configuration-properties/#sparkdeployspreadout","text":"","title":"spark.deploy.spreadOut <p>Controls whether standalone <code>Master</code> should perform round-robin scheduling across worker nodes (spreading out each app among all the nodes) instead of trying to consolidate each app onto a small number of nodes</p> <p>Default: <code>true</code></p>"},{"location":"configuration-properties/#sparkmasterrestenabled","text":"","title":"spark.master.rest.enabled <p>Default: <code>false</code></p> <p>Used when:</p> <ul> <li><code>Master</code> standalone application is requested to onStart</li> </ul>"},{"location":"configuration-properties/#sparkmasterrestport","text":"","title":"spark.master.rest.port <p>Default: <code>6066</code></p> <p>Used when:</p> <ul> <li><code>Master</code> standalone application is requested to onStart</li> <li><code>StandaloneSubmitRequestServlet</code> is requested to <code>buildDriverDescription</code></li> </ul>"},{"location":"configuration-properties/#sparkworkerresourcesfile","text":"","title":"spark.worker.resourcesFile <p>(internal) Path to a file containing the resources allocated to the worker. The file should be formatted as a JSON array of ResourceAllocation objects. Only used internally in standalone mode.</p> <p>Default: (undefined)</p> <p>Used when:</p> <ul> <li><code>LocalSparkCluster</code> is requested to start</li> <li><code>Worker</code> standalone application is launched</li> </ul>"},{"location":"logging/","text":"<p>Spark uses log4j for logging.</p>  <p>Note</p> <p>Learn more on Spark Logging in The Internals of Apache Spark online book.</p>","title":"Logging"},{"location":"overview/","text":"<p>Spark Standalone (Standalone Cluster) is Spark's own built-in cluster environment. Since Spark Standalone is available in the default distribution of Apache Spark it is the easiest way to run your Spark applications in a clustered environment in many cases.</p> <p>Standalone Master (standalone Master) is the resource manager for the Spark Standalone cluster.</p> <p>Standalone Worker is a worker in a Spark Standalone cluster. There can be one or many workers in a standalone cluster.</p> <p>In Standalone cluster mode Spark allocates resources based on cores. By default, an application will grab all the cores in the cluster.</p> <p>Standalone cluster mode is subject to the constraint that only one executor can be allocated on each worker per application.</p> <p>A Spark Standalone cluster is available using <code>spark://</code>-prefixed master URL.</p>","title":"Spark Standalone"},{"location":"overview/#round-robin-scheduling-across-nodes","text":"<p>If enabled (using spark.deploy.spreadOut), standalone Master attempts to spread out an application's executors on as many workers as possible (instead of trying to consolidate it onto a small number of nodes).</p>","title":"Round-robin Scheduling Across Nodes"},{"location":"overview/#scheduleexecutorsonworkers","text":"<pre><code>scheduleExecutorsOnWorkers(\n  app: ApplicationInfo,\n  usableWorkers: Array[WorkerInfo],\n  spreadOutApps: Boolean): Array[Int]\n</code></pre> <p><code>scheduleExecutorsOnWorkers</code> schedules executors on workers.</p>","title":"scheduleExecutorsOnWorkers"},{"location":"overview/#spark_worker_instances-and-spark_worker_cores","text":"<p>There is really no need to run multiple workers per machine in Spark 1.5 (perhaps in 1.4, too). You can run multiple executors on the same machine with one worker.</p> <p>Use <code>SPARK_WORKER_INSTANCES</code> (default: <code>1</code>) in <code>spark-env.sh</code> to define the number of worker instances.</p> <p>If you use <code>SPARK_WORKER_INSTANCES</code>, make sure to set <code>SPARK_WORKER_CORES</code> explicitly to limit the cores per worker, or else each worker will try to use all the cores.</p> <p>You can set up the number of cores as an command line argument when you start a worker daemon using <code>--cores</code>.</p>","title":"SPARK_WORKER_INSTANCES (and SPARK_WORKER_CORES)"},{"location":"overview/#multiple-executors-per-worker-in-standalone-mode","text":"<p>CAUTION: It can be a duplicate of the above section.</p> <p>It is possible to start multiple executors in a single JVM process of a worker.</p> <p>To launch multiple executors on a machine you start multiple standalone workers, each with its own JVM. It introduces unnecessary overhead due to these JVM processes, provided that there are enough cores on that worker.</p> <p>If you are running Spark in standalone mode on memory-rich nodes it can be beneficial to have multiple worker instances on the same node as a very large heap size has two disadvantages:</p> <ul> <li>Garbage collector pauses can hurt throughput of Spark jobs.</li> <li>Heap size of &gt;32 GB can\u2019t use CompressedOoops. So 35 GB is actually less than 32 GB.</li> </ul> <p>Mesos and YARN can, out of the box, support packing multiple, smaller executors onto the same physical host, so requesting smaller executors doesn\u2019t mean your application will have fewer overall resources.</p>","title":"Multiple executors per worker in Standalone mode"},{"location":"overview/#sparkdeployschedulerbackend","text":"<p><code>SparkDeploySchedulerBackend</code> is the xref:scheduler:SchedulerBackend.md[Scheduler Backend] for Spark Standalone, i.e. it is used when you xref:ROOT:SparkContext.md#creating-instance[create a SparkContext] using <code>spark://</code> master URL.</p> <p>It is a specialized xref:scheduler:CoarseGrainedSchedulerBackend.md[CoarseGrainedSchedulerBackend] that uses &lt;&gt; and is a <code>AppClientListener</code>. <p>.SparkDeploySchedulerBackend.start() (while SparkContext starts) image::SparkDeploySchedulerBackend-AppClient-start.png[align=\"center\"]</p> <p>CAUTION: FIXME <code>AppClientListener</code> &amp; <code>ApplicationDescription</code></p> <p>It uses &lt;&gt; to talk to executors.","title":"SparkDeploySchedulerBackend"},{"location":"overview/#appclient","text":"<p><code>AppClient</code> is an interface to allow Spark applications to talk to a Standalone cluster (using a RPC Environment). It takes an RPC Environment, a collection of master URLs, a <code>ApplicationDescription</code>, and a <code>AppClientListener</code>.</p> <p>It is solely used by &lt;&gt;. <p><code>AppClient</code> registers AppClient RPC endpoint (using <code>ClientEndpoint</code> class) to a given RPC Environment.</p> <p><code>AppClient</code> uses a daemon cached thread pool (<code>askAndReplyThreadPool</code>) with threads' name in the format of <code>appclient-receive-and-reply-threadpool-ID</code>, where <code>ID</code> is a unique integer for asynchronous asks and replies. It is used for requesting executors (via <code>RequestExecutors</code> message) and kill executors (via <code>KillExecutors</code>).</p> <p><code>sendToMaster</code> sends one-way <code>ExecutorStateChanged</code> and <code>UnregisterApplication</code> messages to master.</p>","title":"AppClient"},{"location":"overview/#initialization-appclientstart-method","text":"<p>When AppClient starts, <code>AppClient.start()</code> method is called that merely registers &lt;&gt;.","title":"Initialization - AppClient.start() method"},{"location":"overview/#appclient-rpc-endpoint","text":"<p>AppClient RPC endpoint is started as part of &lt;&gt; (that is in turn part of &lt;&gt;). <p>It is a <code>ThreadSafeRpcEndpoint</code> that knows about the RPC endpoint of the primary active standalone Master (there can be a couple of them, but only one can be active and hence primary).</p> <p>When it starts, it sends &lt;&gt; message to register an application and itself.","title":"AppClient RPC Endpoint"},{"location":"overview/#registerapplication-rpc-message","text":"<p>An AppClient registers the Spark application to a single master (regardless of the number of the standalone masters given in the master URL).</p> <p></p> <p>It uses a dedicated thread pool appclient-register-master-threadpool to asynchronously send <code>RegisterApplication</code> messages, one per standalone master.</p> <pre><code>Connecting to master spark://localhost:7077...\n</code></pre> <p>An AppClient tries connecting to a standalone master 3 times every 20 seconds per master before giving up. They are not configurable parameters.</p> <p>The appclient-register-master-threadpool thread pool is used until the registration is finished, i.e. AppClient is connected to the primary standalone Master or the registration fails. It is then <code>shutdown</code>.</p>","title":"RegisterApplication RPC message"},{"location":"overview/#registeredapplication-rpc-message","text":"<p><code>RegisteredApplication</code> is a one-way message from the primary master to confirm successful application registration. It comes with the application id and the master's RPC endpoint reference.</p> <p>The <code>AppClientListener</code> gets notified about the event via <code>listener.connected(appId)</code> with <code>appId</code> being an application id.</p>","title":"RegisteredApplication RPC message"},{"location":"overview/#applicationremoved-rpc-message","text":"<p><code>ApplicationRemoved</code> is received from the primary master to inform about having removed the application. AppClient RPC endpoint is stopped afterwards.</p> <p>It can come from the standalone Master after a kill request from Web UI, application has finished properly or the executor where the application was still running on has been killed, failed, lost or exited.</p>","title":"ApplicationRemoved RPC message"},{"location":"overview/#executoradded-rpc-message","text":"<p><code>ExecutorAdded</code> is received from the primary master to inform about...FIXME</p> <p>CAUTION: FIXME the message</p> <pre><code>Executor added: %s on %s (%s) with %d cores\n</code></pre>","title":"ExecutorAdded RPC message"},{"location":"overview/#executorupdated-rpc-message","text":"<p><code>ExecutorUpdated</code> is received from the primary master to inform about...FIXME</p> <p>CAUTION: FIXME the message</p> <pre><code>Executor updated: %s is now %s%s\n</code></pre>","title":"ExecutorUpdated RPC message"},{"location":"overview/#masterchanged-rpc-message","text":"<p><code>MasterChanged</code> is received from the primary master to inform about...FIXME</p> <p>CAUTION: FIXME the message</p> <pre><code>Master has changed, new master is at\n</code></pre>","title":"MasterChanged RPC message"},{"location":"overview/#stopappclient-rpc-message","text":"<p><code>StopAppClient</code> is a reply-response message from the SparkDeploySchedulerBackend to stop the AppClient after the SparkContext has been stopped (and so should the running application on the standalone cluster).</p> <p>It stops the AppClient RPC endpoint.</p>","title":"StopAppClient RPC message"},{"location":"overview/#requestexecutors-rpc-message","text":"<p><code>RequestExecutors</code> is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to request executors for the application.</p>","title":"RequestExecutors RPC message"},{"location":"overview/#killexecutors-rpc-message","text":"<p><code>KillExecutors</code> is a reply-response message from the SparkDeploySchedulerBackend that is passed on to the master to kill executors assigned to the application.</p>","title":"KillExecutors RPC message"}]}